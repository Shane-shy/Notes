# 对比学习InfoNCE损失

## 介绍

**核心思想**：将正样本拉近，将负样本推远。通俗来讲，让相似的样本在特征空间中靠得更近，不相似的样本在特征空间中距离更远。主要用与**自监督或无监督**场景下的特征学习，然后用于下游任务。

|     对比学习     |     强化学习     |
| :--------------: | :--------------: |
| 学习好的**特征** | 学习好的**策略** |

常用的使用场景有：

- 计算机视觉
  - 图像检索：**拉近正样本**，故很容易检索到相似的图片
  - 目标检测与分割：在少量标签数据上微调即可达到或超过监督学习的性能。有点提高泛化性的感觉
- 自然语言处理
  - 句子/文档表征：对同一段落的不同截断、同义改写、翻译等进行对比学习，**拉近正样本**，可得到通用句向量，用于语义检索等下游任务
  - 文本聚类与分类：在无标签文本库上预训练，再在小规模标注数据上微调，大幅提升文本分类与意图识别的效果
- 推荐系统
  - 冷启动问题：在新用户或新物品无／少交互数据时，利用对比学习的预训练特征帮助快速建立推荐模型
- 医学影像
  - 少样本诊断：培养一个能看懂医学影像的人才很难，而且需求逐年增加，因此现实中，医学影像的标注十分困难。对比学习可在大量未标注的医学影像上预训练，提高疾病检测和分割的准确率
  - 域泛化：不同医院、不同设备得到的影像风格差异很大，会导致模型性能大幅退化。对比学习的视图不变性一定程度上有助于域泛化
- 工业与安防
  - 异常检测：拉近正样本。检测到与正样本特征分布较远的，则标记为异常

常用场景可大致归纳为：

1. 标注数据少，数据标注难，甚至没法标注
2. 提升通用性，泛化性

## InfoNCE损失

这是一个经典的对比损失函数，现在我们从数学公式的角度上解释是如何实现“拉近正样本，推远负样本”的效果的。

假设对一批大小为$N$的样本，每个样本经两种不同增强得到一对表示${(z_i,z_i^+)}_{i=1}^N$，其中$ z_i=f(x_i)$、$z_i^+=f(\bar{x_i})$。定义余弦相似度：$sim(u,v)  =  \frac{u^\top v}{\Vert u \Vert \vert v \Vert}$

那么第 $i$个样本的 InfoNCE 损失为
$$
\mathcal L_i = - \log \frac{exp(sim(Z_i,Z_i^+) / \tau)}{\sum_{j=1}^N exp(sim(Z_i,Z_j^+)/ \tau)}
$$
其中 $\tau>0$ 是温度系数。

- **正样本对（$Z_i$与 $Z_i^+$）** 出现在分子中：要最大化该项，就必须让 $sim(z_i,z_i^+)$ 尽可能大，也就是将正样本对在特征空间“拉近”。
- **负样本对（$z_i$与所有 $z_j^+$,$j \neq i$ ）** 出现在分母中：当分母里的项变大（即负样本对相似度变大）时，整个分式变小，损失 $\mathcal L+i$ 会增大。为了最小化 $\mathcal L_i$，模型就必须把所有负样本对的相似度 $sim(z_i,z_j^+)$“推远”，降低分母，从而减少干扰。
- 温度系数作用
  - $\tau$很小（如0.05 - 0.1）：分母中最大的几项数据会被**指数级别**放得极大，导致softmax分布尖锐——模型更加专注于区分最难的负样本。
  - $\tau$很大（比如0.5 - 1）：分子分母之间的差距会相对变小，在指数的作用下，比例差距相对较小，所以softmax分布更平滑，更均匀。

最后取整个批次的平均：
$$
\mathcal L_{InfoNCE} = \frac{1}{N}\sum_{i=1}^N \mathcal L_i
$$
