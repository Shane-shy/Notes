# 熵、交叉熵、KL散度

## 熵

> 描述自身分布的不确定性。

1. 熵是干什么的？

熵是度量不确定性的。如果某个东西越不确定，那么我就需要用更多的信息才能弄清楚它。此时，熵就很大。

- 事件越均匀，不确定性越大

举个例子，从1-100个数字里等概率抽取一个数字。由于是等概率的，所以不确定性很大，即熵很大。如果从2个红苹果和1个绿苹果里，抽中一个红苹果。相对来说，抽中红苹果的不确定性要小一些，即熵要小一些。这引导出一个结论——**熵随着概率越平均而变大。**

- 独立事件信息可加

要确定两个独立事件A和B需要的信息是多少呢？既然独立，那么互不影响，所以需要两次信息相加。即$H(A,B)=H(A)+H(B)$。在常见函数里，能满足这条性质的，只有对数函数。$log(AB)=log(A)+log(B)$。

- 事件越可能，提供的信息量越小

当一件事情大概率发生的时候，我们不会因为这件事发生了而感到惊讶，所以事件越可能发生，能提供的信息就越少。我们用$I(p)$（information）表示一个概率为p的事件带来的信息量。思考一下极端情况。概率越大，信息量越小，那么如果概率为1，信息量应该为多少呢？对的，应该为1。再考虑一下另外一个极端情况，当概率越来越小，信息量就会越来越大。如果概率为0，那么能提供的信息量应该就是无穷大的。基于此，再结合上面的独立事件信息可加性质，我们能得到：$I(p)=-\log(p)$。因为概率p只能在[0,1]之间，所以$\log(p)$一定为负。为了保证信息量不为负，添加了负号。（大不了不提供信息，怎么会倒提供信息呢？是吧？:) ）

- 求信息量的期望

如果不止一件事情，而是很多件事情。那么它们能带来的信息量会是多少呢？这个时候，就要求一个平均信息量（期望）。如果一件事情的发生概率是$p_i$，能提供的信息量是$I(p_i)$，那么这件事情能提供的信息量期望就是$p_iI(p_i)$。现在只需要把所有事情的信息量加起来，就得到：
$$
H(p)=-\sum_i p_i \log(p_i)
$$
Congratulations。我们得到了信息熵的公式定义。

## 交叉熵

> 用Q分布去描述P分布

在现实生活中，我们只能知道一件事情的先验概率，而不能知道一件事情的后验概率。举个例子，我们抛掷一枚硬币。在抛之前，我们知道结果为正面的概率是0.5，但是我们抛十次，最终的结果一定会是0.5吗？不一定。抛之前就能知道的概率就是先验概率；抛之后才能知道的概率就是后验概率。

如果要判断一个模型的分类性能，就是判断模型输出的概率与真实的概率是否接近。也就是说在用模型输出的概率分布去描述真实的概率分布。我们希望的是模型的输出概率分布尽可能跟真实的概率分布一致，那么就能预测准确啦。所以就有了交叉熵：
$$
H(p,q)=-\sum_i p_i \log(q_i)
$$
在分类任务中，标签$y$通常是独热编码，即[0,0,0,1,0,0…,0]。只有真实标签的位置会是1，其他都是0。那么交叉熵计算结果会是$H(p,q)=- \log(q_i)$。

## KL散度

> 用Q分布去描述P分布，需要多付出的信息量

我们已经有了交叉熵，得到了用Q分布去描述P分布的信息量。毕竟是用Q分布去描述的P分布，所以会多付出一些信息量。要多付出多少呢？多付出的这部分就是KL散度。
$$
D_{\text KL}(P \rVert Q)=H(P,Q)-H(P)
$$
所以KL散度的本质是两个熵的比较，是两个分布的差异。

聪明的你一定看出来了，$H(P,Q)=D_{\text KL}(P \rVert Q)+H(P)$，P分布的熵是固定的，那么把交叉熵作为优化目标进行优化，是不是就相当于对KL散度进行优化呢？答案为是的 :)。所以KL散度没有那么神秘。